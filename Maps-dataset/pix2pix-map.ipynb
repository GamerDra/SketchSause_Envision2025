{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:20.399082Z",
     "iopub.status.busy": "2025-05-23T10:46:20.398790Z",
     "iopub.status.idle": "2025-05-23T10:46:23.617339Z",
     "shell.execute_reply": "2025-05-23T10:46:23.616759Z",
     "shell.execute_reply.started": "2025-05-23T10:46:20.399058Z"
    },
    "papermill": {
     "duration": 9.487266,
     "end_time": "2025-05-19T14:53:08.442393",
     "exception": false,
     "start_time": "2025-05-19T14:52:58.955127",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import standard libraries and PyTorch modules for data handling, image processing, \n",
    "# neural network building, optimization, and training utilities.\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torchvision.utils import save_image\n",
    "from torch_fidelity import calculate_metrics\n",
    "\n",
    "from IPython.display import clear_output  # For clearing Jupyter output during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003481,
     "end_time": "2025-05-19T14:53:08.450006",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.446525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:28.874620Z",
     "iopub.status.busy": "2025-05-23T10:46:28.874177Z",
     "iopub.status.idle": "2025-05-23T10:46:28.878798Z",
     "shell.execute_reply": "2025-05-23T10:46:28.878166Z",
     "shell.execute_reply.started": "2025-05-23T10:46:28.874594Z"
    },
    "papermill": {
     "duration": 0.009129,
     "end_time": "2025-05-19T14:53:08.462649",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.453520",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # Number of image pairs processed in one forward/backward pass\n",
    "image_size = (256, 256)  # Target size of input and output images (height, width)\n",
    "\n",
    "lr = 1e-4  # Learning rate for the optimizer\n",
    "betas = (0.0, 0.9)  # Beta parameters for the Adam optimizer (commonly used in GANs)\n",
    "\n",
    "disc_iter = 3  # Number of discriminator updates per generator update (used in some training regimes)\n",
    "\n",
    "lambda_gp = 10  # Gradient penalty coefficient (used in WGAN-GP or other regularized GANs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003244,
     "end_time": "2025-05-19T14:53:08.469441",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.466197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:30.985713Z",
     "iopub.status.busy": "2025-05-23T10:46:30.985381Z",
     "iopub.status.idle": "2025-05-23T10:46:30.989412Z",
     "shell.execute_reply": "2025-05-23T10:46:30.988824Z",
     "shell.execute_reply.started": "2025-05-23T10:46:30.985691Z"
    },
    "papermill": {
     "duration": 0.008427,
     "end_time": "2025-05-19T14:53:08.481192",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.472765",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_directory = \"/kaggle/input/pix2pix-dataset/maps/maps/train\"\n",
    "# Path to the training set containing paired satellite-map images\n",
    "\n",
    "val_directory = \"/kaggle/input/pix2pix-dataset/maps/maps/val\"\n",
    "# Path to the validation set for evaluating model performance during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:31.149368Z",
     "iopub.status.busy": "2025-05-23T10:46:31.149118Z",
     "iopub.status.idle": "2025-05-23T10:46:31.154011Z",
     "shell.execute_reply": "2025-05-23T10:46:31.153327Z",
     "shell.execute_reply.started": "2025-05-23T10:46:31.149342Z"
    },
    "papermill": {
     "duration": 0.009202,
     "end_time": "2025-05-19T14:53:08.493792",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.484590",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define image preprocessing transformations for the input (e.g., satellite) and target (e.g., map) images\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),  # Resize the input image to the specified size (e.g., 256x256)\n",
    "    transforms.ColorJitter(0.1),    # Apply slight random changes in brightness, contrast, saturation\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n",
    "    # Normalize image pixel values from [0, 1] to [-1, 1] (standard for GANs)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),  # Resize the target image to match the input size\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n",
    "    # Normalize target image pixel values to the same range [-1, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:31.362580Z",
     "iopub.status.busy": "2025-05-23T10:46:31.362259Z",
     "iopub.status.idle": "2025-05-23T10:46:31.368744Z",
     "shell.execute_reply": "2025-05-23T10:46:31.368016Z",
     "shell.execute_reply.started": "2025-05-23T10:46:31.362557Z"
    },
    "papermill": {
     "duration": 0.009981,
     "end_time": "2025-05-19T14:53:08.507253",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.497272",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class map_dataset(Dataset):\n",
    "    def __init__(self, dir, input_transform, target_transfrom):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir (str): Directory containing the dataset images.\n",
    "            input_transform (callable): Transformations to apply to the input (e.g., satellite) images.\n",
    "            target_transfrom (callable): Transformations to apply to the target (e.g., map) images.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dir = dir\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transfrom\n",
    "        self.list_files = os.listdir(self.dir)  # List of image filenames in the directory\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of image pairs\n",
    "        return len(self.list_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image from the dataset\n",
    "        image_path = os.path.join(self.dir, self.list_files[index])\n",
    "        image = transforms.ToTensor()(Image.open(image_path))  # Convert image to tensor\n",
    "\n",
    "        # Split the image into input (X) and target (y)\n",
    "        # Assumes original image is a concatenation of input and target images side-by-side\n",
    "        X = image[:, :, :600]       # Left half (e.g., satellite image)\n",
    "        y = image[:, :, 600:1200]   # Right half (e.g., map image)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.input_transform:\n",
    "            X = self.input_transform(X)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load training and validation datasets with corresponding transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:34.274062Z",
     "iopub.status.busy": "2025-05-23T10:46:34.273769Z",
     "iopub.status.idle": "2025-05-23T10:46:34.279377Z",
     "shell.execute_reply": "2025-05-23T10:46:34.278658Z",
     "shell.execute_reply.started": "2025-05-23T10:46:34.274039Z"
    },
    "papermill": {
     "duration": 0.043301,
     "end_time": "2025-05-19T14:53:08.553957",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.510656",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = map_dataset(train_directory, input_transform, target_transform)\n",
    "val_dataset = map_dataset(val_directory, target_transform, target_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataLoaders for training and validation datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:34.604358Z",
     "iopub.status.busy": "2025-05-23T10:46:34.603781Z",
     "iopub.status.idle": "2025-05-23T10:46:34.608128Z",
     "shell.execute_reply": "2025-05-23T10:46:34.607530Z",
     "shell.execute_reply.started": "2025-05-23T10:46:34.604335Z"
    },
    "papermill": {
     "duration": 0.008869,
     "end_time": "2025-05-19T14:53:08.566493",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.557624",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003295,
     "end_time": "2025-05-19T14:53:08.573431",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.570136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Disciminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:34.984605Z",
     "iopub.status.busy": "2025-05-23T10:46:34.984242Z",
     "iopub.status.idle": "2025-05-23T10:46:34.989509Z",
     "shell.execute_reply": "2025-05-23T10:46:34.988834Z",
     "shell.execute_reply.started": "2025-05-23T10:46:34.984573Z"
    },
    "papermill": {
     "duration": 0.009137,
     "end_time": "2025-05-19T14:53:08.585830",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.576693",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dis_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator block with Conv2D, BatchNorm, and LeakyReLU activation.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        stride (int): Stride size for convolution.\n",
    "\n",
    "    Forward pass applies a 4x4 convolution with reflection padding,\n",
    "    followed by batch normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      kernel_size=4, stride=stride, padding=1, padding_mode=\"reflect\"),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:35.209303Z",
     "iopub.status.busy": "2025-05-23T10:46:35.208638Z",
     "iopub.status.idle": "2025-05-23T10:46:35.214172Z",
     "shell.execute_reply": "2025-05-23T10:46:35.213534Z",
     "shell.execute_reply.started": "2025-05-23T10:46:35.209279Z"
    },
    "papermill": {
     "duration": 0.009501,
     "end_time": "2025-05-19T14:53:08.598699",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.589198",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator for pix2pix.\n",
    "\n",
    "    Combines input and target images (channel-wise), then passes through a series\n",
    "    of convolutional layers to classify local image patches as real or fake.\n",
    "\n",
    "    Input:\n",
    "        X (Tensor): Input image (e.g., satellite)\n",
    "        y (Tensor): Target image (e.g., map)\n",
    "    \n",
    "    Output:\n",
    "        Tensor: Discriminator prediction (patch-level real/fake scores)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=64, kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),  # 128x128\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            Dis_Block(64, 128, 2),   # 64x64\n",
    "            Dis_Block(128, 256, 2),  # 32x32\n",
    "            Dis_Block(256, 512, 1),  # 31x31\n",
    "\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\")  # 30x30\n",
    "        )\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        X = torch.cat([X, y], dim=1)\n",
    "        return self.layers(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003215,
     "end_time": "2025-05-19T14:53:08.605264",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.602049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generator Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:43.494176Z",
     "iopub.status.busy": "2025-05-23T10:46:43.493906Z",
     "iopub.status.idle": "2025-05-23T10:46:43.500568Z",
     "shell.execute_reply": "2025-05-23T10:46:43.499896Z",
     "shell.execute_reply.started": "2025-05-23T10:46:43.494157Z"
    },
    "papermill": {
     "duration": 0.010153,
     "end_time": "2025-05-19T14:53:08.618769",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.608616",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block for the U-Net generator.\n",
    "\n",
    "    Applies a 4x4 Conv2D with stride 2, followed by BatchNorm and LeakyReLU.\n",
    "    Used to downsample feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.block(X)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block for the U-Net generator.\n",
    "\n",
    "    Applies a 4x4 transposed convolution to upsample, followed by BatchNorm and ReLU.\n",
    "    Optionally applies dropout for regularization.\n",
    "    \n",
    "    Args:\n",
    "        dropout (bool): If True, applies Dropout(0.5) after upsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout=False):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.block(X)\n",
    "        return self.dropout_layer(X) if self.dropout else X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:46.864527Z",
     "iopub.status.busy": "2025-05-23T10:46:46.863834Z",
     "iopub.status.idle": "2025-05-23T10:46:46.874524Z",
     "shell.execute_reply": "2025-05-23T10:46:46.873833Z",
     "shell.execute_reply.started": "2025-05-23T10:46:46.864493Z"
    },
    "papermill": {
     "duration": 0.013377,
     "end_time": "2025-05-19T14:53:08.635923",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.622546",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net based generator for the pix2pix model.\n",
    "\n",
    "    Encodes the input image into deep features using downsampling blocks,\n",
    "    then decodes it with upsampling blocks and skip connections for\n",
    "    high-resolution output.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels (default is 3 for RGB images).\n",
    "    \n",
    "    Output:\n",
    "        Tensor: Generated image (same shape as input, with Tanh activation).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder: downsampling blocks\n",
    "        self.de1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.ReLU()\n",
    "        )  # 128x128\n",
    "        self.de2 = DownBlock(64, 128)     # 64x64\n",
    "        self.de3 = DownBlock(128, 256)    # 32x32\n",
    "        self.de4 = DownBlock(256, 512)    # 16x16\n",
    "        self.de5 = DownBlock(512, 512)    # 8x8\n",
    "        self.de6 = DownBlock(512, 512)    # 4x4\n",
    "        self.de7 = DownBlock(512, 512)    # 2x2\n",
    "        self.de8 = nn.Sequential(         # 1x1\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder: upsampling(with skip connections)\n",
    "        self.up1 = UpBlock(512, 512, True)           # 2x2\n",
    "        self.up2 = UpBlock(512*2, 512, True)         # 4x4\n",
    "        self.up3 = UpBlock(512*2, 512, True)         # 8x8\n",
    "        self.up4 = UpBlock(512*2, 512, True)         # 16x16\n",
    "        self.up5 = UpBlock(512*2, 256)               # 32x32\n",
    "        self.up6 = UpBlock(256*2, 128)               # 64x64\n",
    "        self.up7 = UpBlock(128*2, 64)                # 128x128\n",
    "        self.up8 = nn.Sequential(                    # 256x256\n",
    "            nn.ConvTranspose2d(64*2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downsampling\n",
    "        d1 = self.de1(x)\n",
    "        d2 = self.de2(d1)\n",
    "        d3 = self.de3(d2)\n",
    "        d4 = self.de4(d3)\n",
    "        d5 = self.de5(d4)\n",
    "        d6 = self.de6(d5)\n",
    "        d7 = self.de7(d6)\n",
    "        d8 = self.de8(d7)\n",
    "\n",
    "        # Upsampling(with skip connections)\n",
    "        up1 = self.up1(d8)\n",
    "        up2 = self.up2(torch.cat([up1, d7], dim=1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], dim=1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], dim=1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], dim=1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], dim=1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], dim=1))\n",
    "        return self.up8(torch.cat([up7, d1], dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003213,
     "end_time": "2025-05-19T14:53:08.642488",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.639275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:50.523809Z",
     "iopub.status.busy": "2025-05-23T10:46:50.523530Z",
     "iopub.status.idle": "2025-05-23T10:46:50.555027Z",
     "shell.execute_reply": "2025-05-23T10:46:50.554234Z",
     "shell.execute_reply.started": "2025-05-23T10:46:50.523788Z"
    },
    "papermill": {
     "duration": 0.052504,
     "end_time": "2025-05-19T14:53:08.698303",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.645799",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available, otherwise fallback to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:46:52.875568Z",
     "iopub.status.busy": "2025-05-23T10:46:52.874831Z",
     "iopub.status.idle": "2025-05-23T10:46:53.546394Z",
     "shell.execute_reply": "2025-05-23T10:46:53.545608Z",
     "shell.execute_reply.started": "2025-05-23T10:46:52.875532Z"
    },
    "papermill": {
     "duration": 0.722778,
     "end_time": "2025-05-19T14:53:09.424842",
     "exception": false,
     "start_time": "2025-05-19T14:53:08.702064",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(device)  # Initialize discriminator and move to device\n",
    "gen = Generator().to(device)       # Initialize generator and move to device\n",
    "\n",
    "optimizer_disc = optim.Adam(disc.parameters(), lr=lr, betas=betas)  # Optimizer for discriminator\n",
    "optimizer_gen = optim.Adam(gen.parameters(), lr=lr, betas=betas)    # Optimizer for generator\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with logits (used for GAN loss)\n",
    "l1 = nn.L1Loss()                    # L1 loss for pixel-wise similarity between generated and real images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:41:21.077443Z",
     "iopub.status.busy": "2025-05-23T10:41:21.077183Z",
     "iopub.status.idle": "2025-05-23T10:41:21.082963Z",
     "shell.execute_reply": "2025-05-23T10:41:21.082377Z",
     "shell.execute_reply.started": "2025-05-23T10:41:21.077424Z"
    },
    "papermill": {
     "duration": 0.009884,
     "end_time": "2025-05-19T14:53:09.438429",
     "exception": false,
     "start_time": "2025-05-19T14:53:09.428545",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, input_image, device):\n",
    "    # Interpolate between real and fake images\n",
    "    alpha = torch.rand(real.size(0), 1, 1, 1).to(real.device)\n",
    "    interpolated = real * alpha + fake * (1 - alpha)\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    # Get critic scores for interpolated images\n",
    "    mixed_scores = critic(input_image, interpolated)\n",
    "\n",
    "    # Compute gradients of scores w.r.t. interpolated images\n",
    "    grad_outputs = torch.ones_like(mixed_scores)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=mixed_scores,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Flatten gradients and compute gradient penalty\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return penalty  # Enforces Lipschitz constraint for WGAN-GP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:53:09.458477Z",
     "iopub.status.busy": "2025-05-19T14:53:09.458199Z",
     "iopub.status.idle": "2025-05-19T14:53:09.462934Z",
     "shell.execute_reply": "2025-05-19T14:53:09.462303Z"
    },
    "papermill": {
     "duration": 0.009713,
     "end_time": "2025-05-19T14:53:09.463945",
     "exception": false,
     "start_time": "2025-05-19T14:53:09.454232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_losses(G_train_losses, D_train_losses, L1_train_losses,\n",
    "                G_test_losses, D_test_losses, L1_test_losses):\n",
    "    # Clear previous plot output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Plot training and testing losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(G_train_losses, label='Generator Train Loss')\n",
    "    plt.plot(D_train_losses, label='Discriminator Train Loss')\n",
    "    plt.plot(L1_train_losses, label='L1 Train Loss')\n",
    "    plt.plot(G_test_losses, label='Generator Test Loss')\n",
    "    plt.plot(D_test_losses, label='Discriminator Test Loss')\n",
    "    plt.plot(L1_test_losses, label='L1 Test Loss')\n",
    "\n",
    "    # Configure plot\n",
    "    plt.xlabel('Checkpoint every 25 epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Training Losses')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:53:09.483905Z",
     "iopub.status.busy": "2025-05-19T14:53:09.483245Z",
     "iopub.status.idle": "2025-05-19T14:53:09.489132Z",
     "shell.execute_reply": "2025-05-19T14:53:09.488616Z"
    },
    "papermill": {
     "duration": 0.010814,
     "end_time": "2025-05-19T14:53:09.490067",
     "exception": false,
     "start_time": "2025-05-19T14:53:09.479253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(val_loader, gen, epoch, device):\n",
    "    # Generate outputs using the current generator model\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = next(iter(val_loader))\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = targets.to(device)\n",
    "        model_outputs = gen(inputs)\n",
    "\n",
    "    # Plot input, generated, and ground truth images\n",
    "    fig, axs = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    for i in range(4):\n",
    "        axs[0][i].imshow(inputs[i].permute(1, 2, 0).cpu())\n",
    "        axs[1][i].imshow(model_outputs[i].permute(1, 2, 0).cpu())\n",
    "        axs[2][i].imshow(outputs[i].permute(1, 2, 0).cpu())\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.axis('off')\n",
    "    axs[0][0].set_ylabel(\"Input\")\n",
    "    axs[1][0].set_ylabel(\"Generated\")\n",
    "    axs[2][0].set_ylabel(\"Target\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output_image_epoch{epoch}.png\")  # Save comparison as image\n",
    "    gen.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T14:53:09.497916Z",
     "iopub.status.busy": "2025-05-19T14:53:09.497465Z",
     "iopub.status.idle": "2025-05-19T21:02:31.998538Z",
     "shell.execute_reply": "2025-05-19T21:02:31.997752Z"
    },
    "papermill": {
     "duration": 22162.528961,
     "end_time": "2025-05-19T21:02:32.022484",
     "exception": false,
     "start_time": "2025-05-19T14:53:09.493523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "G_test_losses = []\n",
    "D_test_losses = []\n",
    "L1_test_losses = []\n",
    "G_train_losses = []\n",
    "D_train_losses = []\n",
    "L1_train_losses = []\n",
    "\n",
    "epochs = 200  # Total number of training epochs\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Initializing accumulators for losses in this epoch\n",
    "    gen_test_loss, dis_test_loss, l1_test_loss = 0.0, 0.0, 0.0\n",
    "    gen_train_loss, dis_train_loss, l1_train_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Training discriminator multiple times per generator update (disc_iter)\n",
    "        for _ in range(disc_iter):\n",
    "            # Generating fake images without gradient computation for generator\n",
    "            with torch.no_grad():\n",
    "                y_fake = gen(x)\n",
    "\n",
    "            # Computing discriminator output on real and fake images\n",
    "            d_real = disc(x, y)\n",
    "            d_fake = disc(x, y_fake)\n",
    "\n",
    "            # Calculating gradient penalty for WGAN-GP regularization\n",
    "            gp = gradient_penalty(disc, y, y_fake, x, device)\n",
    "\n",
    "            # Wasserstein loss with gradient penalty for discriminator\n",
    "            d_loss = -(torch.mean(d_real) - torch.mean(d_fake)) + lambda_gp * gp\n",
    "\n",
    "            optimizer_disc.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_disc.step()\n",
    "\n",
    "            dis_train_loss += d_loss.item()\n",
    "\n",
    "        y_fake = gen(x)  # Generating fake images with gradients\n",
    "        d_fake = disc(x, y_fake)\n",
    "\n",
    "        # Generator adversarial loss (BCE loss against 'real' labels)\n",
    "        gen_fakeloss = criterion(d_fake, torch.ones_like(d_fake))\n",
    "\n",
    "        # L1 loss encourages pixel-wise similarity between generated and real images\n",
    "        L1_loss = l1(y_fake, y) * 100\n",
    "\n",
    "        # Total generator loss is adversarial + L1 losses\n",
    "        gen_total_loss = gen_fakeloss + L1_loss\n",
    "\n",
    "        optimizer_gen.zero_grad()\n",
    "        gen_total_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        gen_train_loss += gen_total_loss.item()\n",
    "        l1_train_loss += L1_loss.item()\n",
    "\n",
    "    # Average training losses over the entire training dataset\n",
    "    G_train_losses.append(gen_train_loss / len(train_loader))\n",
    "    D_train_losses.append(dis_train_loss / len(train_loader))\n",
    "    L1_train_losses.append(l1_train_loss / len(train_loader))\n",
    "\n",
    "    #Validation Phase\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_fake = gen(x)\n",
    "\n",
    "            # Discriminator loss on validation set using BCE loss\n",
    "            d_real = disc(x, y)\n",
    "            d_fake = disc(x, y_fake.detach())  # detach to avoid generator gradients\n",
    "\n",
    "            d_realloss = criterion(d_real, torch.ones_like(d_real))\n",
    "            d_fakeloss = criterion(d_fake, torch.zeros_like(d_fake))\n",
    "            d_loss = (d_realloss + d_fakeloss) / 2\n",
    "            dis_test_loss += d_loss.item()\n",
    "\n",
    "            # Generator loss on validation set\n",
    "            d_fake = disc(x, y_fake)\n",
    "            gen_fakeloss = criterion(d_fake, torch.ones_like(d_fake))\n",
    "            L1_loss = l1(y_fake, y) * 100\n",
    "            gen_total_loss = gen_fakeloss + L1_loss\n",
    "\n",
    "            gen_test_loss += gen_total_loss.item()\n",
    "            l1_test_loss += L1_loss.item()\n",
    "\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    # Average validation losses over the validation dataset\n",
    "    G_test_losses.append(gen_test_loss / len(val_loader))\n",
    "    D_test_losses.append(dis_test_loss / len(val_loader))\n",
    "    L1_test_losses.append(l1_test_loss / len(val_loader))\n",
    "\n",
    "    # Saving generated sample images and plot losses every 25 epochs\n",
    "    if epoch % 25 == 0:\n",
    "        save_checkpoint(val_loader, gen, epoch, device)\n",
    "        plot_losses(G_train_losses, D_train_losses, L1_train_losses,\n",
    "                    G_test_losses, D_test_losses, L1_test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:02:32.070860Z",
     "iopub.status.busy": "2025-05-19T21:02:32.070639Z",
     "iopub.status.idle": "2025-05-19T21:02:32.448915Z",
     "shell.execute_reply": "2025-05-19T21:02:32.448333Z"
    },
    "papermill": {
     "duration": 0.403859,
     "end_time": "2025-05-19T21:02:32.450251",
     "exception": false,
     "start_time": "2025-05-19T21:02:32.046392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), \"Generator_weights.pth\")      # To save Generator model weights\n",
    "torch.save(disc.state_dict(), \"Discriminator_weights.pth\") # To save Discriminator model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:47:07.654899Z",
     "iopub.status.busy": "2025-05-23T10:47:07.654251Z",
     "iopub.status.idle": "2025-05-23T10:47:07.906757Z",
     "shell.execute_reply": "2025-05-23T10:47:07.906056Z",
     "shell.execute_reply.started": "2025-05-23T10:47:07.654871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"/kaggle/input/generator-weights/Generator_weights.pth\" \n",
    "\n",
    "gen.load_state_dict(torch.load(weights_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "gen.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T10:51:16.050752Z",
     "iopub.status.busy": "2025-05-23T10:51:16.050449Z",
     "iopub.status.idle": "2025-05-23T10:53:14.535121Z",
     "shell.execute_reply": "2025-05-23T10:53:14.534236Z",
     "shell.execute_reply.started": "2025-05-23T10:51:16.050729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9e3c042e974f2883d2570526ebd669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:00<00:00, 350MB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Metrics ---\n",
      "Avg SSIM: 0.7383\n",
      "Avg PSNR: 26.93 dB\n",
      "Avg Gram Matrix Distance: 0.000185\n",
      "FID: 286.07\n"
     ]
    }
   ],
   "source": [
    "# Directories to store images temporarily for FID\n",
    "real_dir = \"real_images\"\n",
    "gen_dir = \"gen_images\"\n",
    "os.makedirs(real_dir, exist_ok=True)\n",
    "os.makedirs(gen_dir, exist_ok=True)\n",
    "\n",
    "# Move generator to eval mode\n",
    "gen.eval()\n",
    "\n",
    "ssim_scores, psnr_scores, gram_diffs = [], [], []\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b, c, h * w)\n",
    "    G = torch.bmm(features, features.transpose(1, 2))  # (b, c, c)\n",
    "    return G / (c * h * w)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (input_img, target_img) in enumerate(tqdm(val_loader)):\n",
    "        input_img = input_img.to(device)       # [B, 3, H, W]\n",
    "        target_img = target_img.to(device)     # [B, 3, H, W]\n",
    "\n",
    "        pred_img = gen(input_img)        # [B, 3, H, W]\n",
    "\n",
    "        for b in range(pred_img.size(0)):\n",
    "            pred = pred_img[b].detach().cpu()\n",
    "            real = target_img[b].detach().cpu()\n",
    "\n",
    "            # Save for FID (denormalized to [0, 1])\n",
    "            save_image((pred + 1) / 2, f\"{gen_dir}/{i*val_loader.batch_size + b}.png\")\n",
    "            save_image((real + 1) / 2, f\"{real_dir}/{i*val_loader.batch_size + b}.png\")\n",
    "\n",
    "            # Convert for metrics (CHW to HWC, [0, 1])\n",
    "            pred_np = ((pred.numpy() + 1) / 2).transpose(1, 2, 0)\n",
    "            real_np = ((real.numpy() + 1) / 2).transpose(1, 2, 0)\n",
    "\n",
    "            # Clamp to avoid nan issues\n",
    "            pred_np = np.clip(pred_np, 0, 1)\n",
    "            real_np = np.clip(real_np, 0, 1)\n",
    "\n",
    "            # SSIM & PSNR\n",
    "            # ssim_scores.append(ssim(pred_np, real_np, channel_axis=-1))\n",
    "            ssim_scores.append(ssim(pred_np, real_np, data_range=1.0, channel_axis=-1))\n",
    "\n",
    "            psnr_scores.append(psnr(real_np, pred_np))\n",
    "\n",
    "            # Gram Matrix Distance\n",
    "            pred_gram = gram_matrix(pred.unsqueeze(0))\n",
    "            real_gram = gram_matrix(real.unsqueeze(0))\n",
    "            gram_diffs.append(F.mse_loss(pred_gram, real_gram).item())\n",
    "\n",
    "# Compute FID\n",
    "fid_result = calculate_metrics(\n",
    "    input1=gen_dir,\n",
    "    input2=real_dir,\n",
    "    fid = True,\n",
    "    metrics=['fid'],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Display Results\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Avg SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "print(f\"Avg PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
    "print(f\"Avg Gram Matrix Distance: {np.mean(gram_diffs):.6f}\")\n",
    "print(f\"FID: {fid_result['frechet_inception_distance']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 34683,
     "sourceId": 47283,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7493807,
     "sourceId": 11919938,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22180.686307,
   "end_time": "2025-05-19T21:02:35.402037",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T14:52:54.715730",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1849f9aa51a34018a5cdf5e7abc1af5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5f3700539f044fca807110017ac41bc2",
        "IPY_MODEL_333e59d0426444d49fa1071c56de0785",
        "IPY_MODEL_3e44418a887e4f80b4f69c96f48f3cf5"
       ],
       "layout": "IPY_MODEL_ddcc1433b75c4dcb8593d4bdf24a13ea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "333e59d0426444d49fa1071c56de0785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c5f13ba3b9404bebb2fed824dbd82aa9",
       "max": 200,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b294f933b6546c4bc0ae6fc85912141",
       "tabbable": null,
       "tooltip": null,
       "value": 200
      }
     },
     "3e44418a887e4f80b4f69c96f48f3cf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d3080f6b24044f0487ccd31d4d0f194f",
       "placeholder": "​",
       "style": "IPY_MODEL_74b39dd4cafc41fab611ab31db495eb2",
       "tabbable": null,
       "tooltip": null,
       "value": " 200/200 [6:09:22&lt;00:00, 109.02s/it]"
      }
     },
     "58211cd970b348a58eec62e14ae1a620": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5f3700539f044fca807110017ac41bc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cc26cf35b2b64d48a8433663f667cf08",
       "placeholder": "​",
       "style": "IPY_MODEL_58211cd970b348a58eec62e14ae1a620",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "6b294f933b6546c4bc0ae6fc85912141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "74b39dd4cafc41fab611ab31db495eb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c5f13ba3b9404bebb2fed824dbd82aa9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc26cf35b2b64d48a8433663f667cf08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3080f6b24044f0487ccd31d4d0f194f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddcc1433b75c4dcb8593d4bdf24a13ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
